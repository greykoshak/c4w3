<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Apply metrics</title>
    <style type="text/css">
        body {
            font-family: verdana, arial, sans-serif;
        }
        b {
            font-size: 125%;
            color: blue;
        }
        i {
            font-size: 110%;
            color: green;
        }
    </style>
</head>
<body>

<div><p>
    В этом видео мы с вами говорим про то как оценивать качество построенных моделей и как разбивать
    наши данные так, чтобы быть уверенными в том что наша модель будет хорошо работать на новых данных
    о которых мы еще не видели, например в реальном мире. Будем работать с вами с библиотекой с sklearn
    и стандартными dataset, с которыми мы уже встречались, например, dataset по раку груди.
    Также импортируем новый модуль metrics, в котором как раз содержатся функции, помогающие нам оценить
    качество работы построенных моделей. И для начала будем работать с логистической регрессией с
    которой уже знакомы. Обучаем нашу логистическую регрессию с помощью метода fit на всех данных и
    посмотрим на accuracy, которая возвращает метод score у нашей модели. Accuracy достаточно неплохое.
    Однако, мы обучаемся на всех данных. Точно так же можно посмотреть на accuracy построив предсказание
    с помощью метода predict на всех наших данных эти предсказания и вызвать у модуля metrics различные
    функции. Например, accuracy_score возвращает accuracy. Если мы запустим ячейку, то у нас accuracy
    будет такое же. Но там существует достаточно большое количество различных метрик как для
    классификаций, так и для регрессий, которые можно вызывать, использовать и смотреть. Например, F1,
    который как то усредняет pressision на recu или площадь под рог кривой. Итак как мы с вами уже
    говорили обучаться на всех данных не всегда здорово, потому что мы не можем достоверно оценить как
    наша модель будет вести себя на каких то новых данных. Например, когда мы запустим нашу модель
    продакшн, было бы здорово обучать наши модели на каких то одних данных и потом смотреть как она
    будет генерелизоваться, то есть обучаться как вести себя на каком то новом наборе данных. Для этого
    нам может помочь model_selection и импортируем оттуда метод функцию train_test_split, а что же
    делает train_test_split. В train_test_split мы передаем наши данные, передаем наши признаки и
    передаем нашу целевую переменнаю и будем разбивать наши данные на тестовой и тренировочную выборку,
    как мы с вами уже говорили. У нас будет тренировочная выборка на которой мы обучаемся, в которую мы
    подбираем параметры нашей модели а потом будем оценивать ее качество уже на тестовой выборке, на
    новых объектах, которые мы еще не видели, чтобы посмотреть как наша модель выучила природную
    зависимость. Мы будем делать это с помощью какого то случайного сида 12, для того, чтобы у нас
    зафиксировало качество раз навсегда. Итак, смотрите мы обучаем точно также нашу логистическую
    регрессию с помощью метода fit, однако передаем в него уже только тренировочные объекты, которые
    составляют примерно 80% нашей выборки в соответствии с параметрами и оцениваем качество нашей
    модели уже на train и на тесте, вызываем module_score на train, получаем 0,96. Однако, на тесте у
    нас качество чуть хуже. Это достаточно логичная ситуация, мы обучались на одних данных а смотрим
    качество немного других. Если у нас отличия небольшое как в данном случае, это не страшно. Если у
    нас был например 0,6, то скорее всего можно считать, что модель у нас переобучилась под
    тренировочную выборку. В данном случае все достаточно неплохо. Давайте посмотрим как можно сделать
    то же самое для задачи регрессии. Здесь отличий немного, однако мы посмотрим на несколько новых
    классов на другие возможности линейной регрессии. Итак, импортируем из linear_model 3 класса, это
    <b>Lasso, Ridge и ElasticNet</b>, которые представляют из себя единую регрессию с различными видами
    регуляризации. Lasso это L1 регулфризация, когда мы добавляем в наши функции ошибки первую норму
    весов, то есть мы добавляем функции сумму модулей весов которые мы в данный момент имеем в нашей
    линейной регресси. Ridge это грибневая регрессия, которая добавляет сумму квадратов весов, ну и
    ElasticNet это и то и другое, мы добавляем как L1 так и L2 регуляризацию. Будем работать с dataset
    boston а загрузим его в инстанцируем классы и в цикле по всем нашим моделям будем делать то же
    самое что делать только что. Развиваем наши данные на тестовую и тренировочную выборку в
    соотношении 8 из 20 на 80% будем обучаться на 20% будем тестировать. Обучаем нашу модель с мощью
    метода <i>fit</i> на train и делаем предсказания на тесте и потом будем смотреть уже метрики для
    регрессии, в данном случае мы используем модуль metrics и смотрим среднюю квадратичную ошибку по
    нашим предсказаниям. Давайте посмотрим как отличаются предсказания для различных моделей. Как мы
    видим лучше всего ведет себя гребневая регрессия в данном случае она показала MSE чуть ниже чем
    остальные функции. Мы также можем работать с другими функциями из модуля metrics, например,
    L2 score, возвращает коэфициент детерминации, который по умолчанию используются в методе score у
    модели. Итак, мы с вами поговорили про то как разбивать наши данные на тренировочною и тестовую
    выборку. Однако, есть еще один полезный метод, полезный подход, который используется очень часто
    для, того чтобы не откладывать какой то набор данных в черный ящик, чтобы потом на нем
    тестироваться, а использовать все данные, однако делается это так, чтобы знать что наша модель
    не переобучается и работает хорошо. Метод называется кросс-валидация и методы функции для того
    чтобы кросс-валидацию. Мы с вами разберем какие то базовые sимпортируем из модуля
    model_selection kFold, класс который позволяет вам делать как раз кросс-валидацию cross_val_score,
    который разберем чуть позже. Работать будем с новым dataset. dataset iris, который тоже
    стандартный, можем посмотреть на его описание, это dataset про цветы, про ирисы, решает задачу
    классификации. У нас есть четыре признака у нашего цветка, это длина липестка, ширина липестка и
    длина чашелистника и его ширина. Мы пытаемся предсказать три разных класса ирисов, оказывается,
    что можно это делать только по этим характеристикам. У нас есть три класса задача классификации
    и мы пытаемся угадать 1 из 3х классов 0, 1 либо 2. Итак, будем решать задачи с помощью
    логистической регрессии. Потому что это классификация и создадим наш объект кросс-валидации с
    помощью класса к kFold будем использовать 5 фолдов. То есть, как работает кросс-валидация еще раз,
    у нас разбивается наша выборка на блоки, в нашем случае на пять блоков, непосредственно у нас
    четыре блока использовали чтобы обучить нашу модель, а пятый блок для того, чтобы оценить качество
    нашей модели. Потом мы берем следующие четыре блока и какой то новый из этих пяти блоков выступает
    в качестве революционного блока и так далее, то есть мы в цикле итерируемся по разным разбиениям.
    А всего у нас пять разбиений. Для того, чтобы использовать кросс-валидацию можно, например, в
    цикле использовать метод split разбить наши данные на 5 фолдов непосредственно пять раз
    использовать каждый из них в качестве тестового блока в данном случае мы передаем в переменной
    тройной тест наши данные и уже обучаем нашу модель, а только на тренировочных данных. Давайте
    это сделаем, обучаемся на тренировочных данных, оцениваем качество на данных уже из тестовой
    выборки, которую мы в данном случае в данный момент на данном разбиении не видели и можем вывести
    качество, в данном случае accuracy у нас немного варьируется, есть хорошие есть плохие. Но конечно
    логичным вариантом было бы усреднять это качество на кросс-валидации и брать среднее. Однако
    конечно интерироваться так по крос-валидации не всегда приятно присваивать эти индексы, было бы
    здорово, чтобы это делалось автоматически. Конечно есть такая функция она называется
    сross_val_score. Мы передаем нашу модель, передаем наши данные, передаем как раз объект
    кросс-валидации на 5 фолдов, говорим, что мы хотим измерять accuracy и можем просто это дело
    запустить и как вы видите наша функция сross_val_score возвращает точно такие же значения, которые
    мы получали в цикле. Если мы усредним с помощью метода mean или мы могли на pine сделать мы
    получаем какое то среднее качество кросс-валидации. Обычно именно его и меряем. Итак, мы с вами
    посмотрели на то, как оценивать можно качество построенных моделей. Существует стандартный модуль
    в sklearn, метод, который содержит в себе большое количество функций, которые можно использовать
    для того, чтобы оценивать качество моделей решать задачи регрессий или классификации. Мы
    посмотрели как можно разбивать наши данные так, чтобы у нас была тренировочная и тестовая выборка
    и мы могли быть уверены, что наша модель как то генереализуется, то есть учитывая природные
    зависимости в данных они не переобучаются. Ну, и научились делать кросс-валидацию, которая очень
    часто используется в реальной жизни, моделях, для того, чтобы оценивать их качество и для того
    чтобы обучаться хорошо на всех данных.
</p></div>

</body>
</html>