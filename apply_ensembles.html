<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Применение ансамблевых моделей</title>
 <style type="text/css">
        body {
            font-family: verdana, arial, sans-serif;
        }
        b {
            font-size: 125%;
            color: blue;
        }
        i {
            font-size: 110%;
            color: green;
        }
    </style>
</head>
<body>

<div><p>
    В этом видео мы с вами коротко поговорим о том, как работать с ансамблевыми моделями.
    А конкретно, со случайным лесом и градиентным бустингом в библиотеке sklearn. Импортируем
    библиотеки для визуализации, для работы с данными. И загрузим уже знакомый вам dataset
    "iris" из модуля datasets. Здесь мы будем решать задачу классификации, то есть,
    пересказывать вид цветка по характеристикам его лепестков. Давайте посмотрим, как эти
    характеристики распределяются в разных классах. Построим pairplot с помощью библиотеки
    seaborn. И здесь мы видим, что у нас синий класс, то есть setosa, линейно разделим. Мы
    можем использовать даже какую-то линейную модель, для того чтобы определить, что наш
    цветок принадлежит именно к этому классу. Однако зеленый и красный класс гораздо более
    сильно похожи друг на друга, поэтому здесь уже нужно строить какую-то не линейную
    зависимость, и строить более сложную модель, например, модель основанную на решающих
    деревьях. Именно это мы и будем делать. Давайте импортируем из модуля ансамбль
    RandomForestClassifier, который позволяет решать задачу квалификации. Однако, конечно,
    с помощью случайного леса можно решать задачу регрессии и соответствующий класс есть в
    этом модуле. Из модуля metrics мы будем импортировать accuracy_score и confusion_matrix,
    которую тоже будем использовать. Будем пользоваться стандартным train_test_split
    разбиением. Давайте дистанцируем наш класс. У нас будет случайный лес, в котором сто
    деревьев и зафиксируем какой-то случайный seed. Разобьем нашу выборку на тестовую и
    тренировочную выборку в соотношении 70 на 30. Передадим туда наши данные и очень важно
    заметить, что у нас появился новый параметр stratify. Очень важно, когда мы решаем задачу
    классификации, сделать так чтобы в нашем разбиении сохранялось распределение классов
    таким образом, чтобы мы не обучались на данных, в которых, например, нет какого-то одного
    класса, который потом появится в тестовой выборке. В таком случае мы никак не сможем
    обучиться и узнать природную зависимость про этот класс. Нужно чтобы соотношение классов
    сохранялось при разбиении. То же самое важно, когда мы делаем кросс-валидацию. В данном
    случае полезно использовать stratify scaffold, а не обычный scaffold. Итак, мы сохраняем
    распределение классов при разбиении и обучаем нашу модель на тренировочной выборке.
    То есть, на x_train. Делаем предсказание мы уже на тестовой выборке и смотрим accuracy
    c помощью стандартной метрики accuracy_score. Accuracy у нас получился 0.89. Давайте
    также посмотрим на confusion_matrix. Посмотрим, какие предсказания мы делаем хорошо, а
    какие плохо. Для этого посчитаем метрику confusion_matrix из стандартного модуля matrix
    и отобразим с помощью seaborn hit map. Как мы видим и правда "sitosa" - это тот самый
    синий класс, достаточно хорошо разделим и мы в нем не ошибаемся. Однако другие два класса
    немного смешиваются, потому что они похожи друг на друга. Также с помощью моделей
    основанных на решающей деревьях всегда очень легко и просто посмотреть на
    feature_importance. То есть, на то какой признак внес наибольший вклад в предсказания.
    У каких признаков наибольшая предсказательная способность. В данном случае, случайный лес
    считает, что лучше всего определить класс цветка по ширине лепестка. Конечно, у моделей
    существуют гораздо больше параметров, чем мы использовали. Можно включать и выключать
    bootstrap, ограничивать, например, глубину дерева, менять количество решающих деревьев и
    так далее. Например, из полезных можно всегда считать oob_score. То есть, смотреть
    качество на тех объектах, которые не использовались при обучении наших решающих деревьев.
    Похожим образом можно также использовать градиентный бустинг. Здесь тоже он решает задачу
    классификации. Тоже можно решать задачу регрессии. Но для ирисов у нас классификация.
    Обучаем у нас 100 наших деревьев. Обучаем нашу модель на train и смотрим качество на
    тесте. И в данном случае качество у нас чуть-чуть получше. Ну, так бывает. Можно также
    посмотреть на качество, на то, как градиентный бустинг считает важностью различных
    признаков. В данном случае мы видим, что качество распределяется немного по другому и
    признаки обладают немного другой важностью. Что логично, потому что это другая модель.
    Из важных параметров, здесь тоже существует несколько принципиальных вещей. Например, мы
    можем видеть, что глубина дерева стандартная - всего три. Потому что в градиентном
    бустинге мы как раз хотим строить неглубокие деревья. Ну и другими параметрами тоже
    можно как-то играть и их варьировать. Итак, мы с вами посмотрели как работать на базовом
    уровне с ансамблевыми моделями. На самом деле в библиотеке sklearn интерфейсы везде
    примерно одинаковые. Поэтому мы всегда вызываем условный fit. Там делаем predict или
    predict_proda и оцениваем качество нашей модели с помощью метрики. И делаем с помощью
    какого-то, например, разбиения на training тест или на кросс-валидацию. Это стандартный
    подход, стандартной практики практически всегда.
</p></div>

</body>
</html>